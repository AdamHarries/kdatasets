{
  "name" : "awrg-slcl-fdp-chunk-512",
  "source" : "#ifndef Tuple2_int_float_DEFINED\n#define Tuple2_int_float_DEFINED\ntypedef struct __attribute__((aligned(4))) {\n  int _0;\n  float _1;\n} Tuple2_int_float;\n#endif\n\nfloat clmin(float a, float b){\n  { return fabs(a) < fabs(b) ? fabs(a) : fabs(b); }\n}\nfloat absadd(float a, float b){\n  { return fabs(a) + fabs(b); }\n}\nfloat doubleMultiplyAdd(float dpRes, float alpha, float rowIdxPair2, float beta){\n  \n{\n  float a = fabs(dpRes) + fabs(alpha);\n  float b = fabs(rowIdxPair2) + fabs(beta);\n  return fabs(a) < fabs(b) ? fabs(a) : fabs(b);\n}\n    \n}\nkernel void KERNEL(const global int* restrict v__78878, const global float* restrict v__78879, const global float* restrict v__78880, const global float* restrict v__78881, float v__78882, float v__78883, global float* v__78897, global int* v__78885, int v_MHeight_2, int v_MWidthC_1, int v_VLength_3){ \n#ifndef WORKGROUP_GUARD\n#define WORKGROUP_GUARD\n#endif\nWORKGROUP_GUARD\n{\n  /* Static local memory */\n  /* Typed Value memory */\n  float v__78887; \n  float v__78889; \n  /* Private Memory */\n  float v__78891_0;\n  \n  float v__78893_0;\n  \n  /* atomic_workgroup_map */\n  {\n    global int* v_work_idx_6531 = v__78885; \n    local int v_w_id_78874; \n    if (get_local_id(0) == 0) {\n      v_w_id_78874 = atomic_inc(v_work_idx_6531); \n    }\n    barrier(CLK_LOCAL_MEM_FENCE);\n    \n    while((v_w_id_78874 < ((v_MHeight_2)/(512)))){\n      for (int v_l_id_78875 = get_local_id(0); v_l_id_78875 < 512; v_l_id_78875 = (v_l_id_78875 + get_local_size(0))) {\n        float v_tmp_78938 = 3.4028235E38f; \n        v__78887 = v_tmp_78938; \n        /* reduce_seq */\n        for (int v_i_78876 = 0; v_i_78876 < v_MWidthC_1; v_i_78876 = (1 + v_i_78876)) {\n          float v_tmp_78939 = 3.4028235E38f; \n          v__78889 = v_tmp_78939; \n          int v_index_78940 = v__78878[(v_i_78876 + (512 * v_MWidthC_1 * v_w_id_78874) + (v_MWidthC_1 * v_l_id_78875))]; \n          if (v_index_78940 < 0) {\n            v__78891_0 = v__78889; \n          } else {\n            if (v_index_78940 >= v_VLength_3) {\n              v__78891_0 = v__78889; \n            } else {\n              v__78891_0 = v__78880[v_index_78940]; \n            }\n          }\n          v__78893_0 = absadd(v__78891_0, v__78879[(v_i_78876 + (512 * v_MWidthC_1 * v_w_id_78874) + (v_MWidthC_1 * v_l_id_78875))]); \n          v__78887 = clmin(v__78893_0, v__78887); \n        }\n        /* end reduce_seq */\n        /* map_seq */\n        /* iteration count is exactly 1, no loop emitted */\n        {\n          int v_i_78877 = 0; \n          v__78897[(v_l_id_78875 + (512 * v_w_id_78874))] = doubleMultiplyAdd(v__78887, v__78882, v__78881[(v_l_id_78875 + (512 * v_w_id_78874))], v__78883); \n        }\n        /* end map_seq */\n      }\n      if (get_local_id(0) == 0) {\n        v_w_id_78874 = atomic_inc(v_work_idx_6531); \n      }\n      barrier(CLK_LOCAL_MEM_FENCE);\n      \n    }\n  }\n  barrier(CLK_GLOBAL_MEM_FENCE);\n  \n}}\n\n",
  "properties" : {
    "outerMap" : "awrg",
    "innerMap" : "slcl",
    "chunkSize" : "512",
    "dotProduct" : "fused"
  },
  "inputArgs" : [ {
    "variable" : "v__78878",
    "addressSpace" : "global",
    "size" : "(4*v_MHeight_2*v_MWidthC_1)"
  }, {
    "variable" : "v__78879",
    "addressSpace" : "global",
    "size" : "(4*v_MHeight_2*v_MWidthC_1)"
  }, {
    "variable" : "v__78880",
    "addressSpace" : "global",
    "size" : "(4*v_VLength_3)"
  }, {
    "variable" : "v__78881",
    "addressSpace" : "global",
    "size" : "(4*v_MHeight_2)"
  }, {
    "variable" : "v__78882",
    "addressSpace" : "private",
    "size" : "4"
  }, {
    "variable" : "v__78883",
    "addressSpace" : "private",
    "size" : "4"
  } ],
  "tempGlobals" : [ {
    "variable" : "v__78885",
    "addressSpace" : "global",
    "size" : "4"
  } ],
  "outputArg" : {
    "variable" : "v__78897",
    "addressSpace" : "global",
    "size" : "(4*v_MHeight_2)"
  },
  "tempLocals" : [ ],
  "paramVars" : [ "MHeight", "MWidthC", "VLength" ],
  "outputSize" : "(4*v_MHeight_2)"
}